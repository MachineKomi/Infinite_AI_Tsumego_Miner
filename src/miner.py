"""
MatchOrchestrator: The Conductor of the Infinite AI Tsumego Miner

This module contains the main mining loop that:
1. Pairs neural agents for adversarial self-play
2. Detects blunders via the High Referee
3. Extracts and saves puzzles with rich metadata

Infinite AI Tsumego Miner
Copyright (C) 2025

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU Affero General Public License as published
by the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.
"""

import os
import sys
import json
import uuid
import time
import hashlib
import argparse
from datetime import datetime

from katago_wrapper import KataGoEngine
from network_bench import NetworkBench
from model_registry import find_referee_model, get_alias, get_model_info
from logger import setup_logging, get_logger, MiningLogger

# =============================================================================
# CONSTANTS
# =============================================================================

if os.name == 'nt':
    KATAGO_BIN = "./assets/katago/katago.exe"
else:
    KATAGO_BIN = "./assets/katago/katago"

MODELS_DIR = "./assets/models"
OUTPUT_DIR = "output"

# Blunder detection thresholds
# A blunder occurs when the current player's winrate drops dramatically
BLUNDER_THRESHOLD_HIGH = 0.90  # Must be winning by at least this much before
BLUNDER_THRESHOLD_LOW = 0.15   # Must drop to at least this after

# Game limits
MAX_MOVES_PER_GAME = 81  # 9x9 board = 81 intersections max
MAX_CONSECUTIVE_PASSES = 2


# =============================================================================
# PUZZLE POSITION HASHER (for deduplication)
# =============================================================================

def hash_position(moves_history):
    """
    Create a hash of the board position for deduplication.
    Uses the move sequence as a proxy (not perfect but fast).
    """
    move_str = "|".join(f"{m[0]}{m[1]}" for m in moves_history)
    return hashlib.md5(move_str.encode()).hexdigest()[:12]


# =============================================================================
# SGF EXPORT
# =============================================================================

def moves_to_sgf_coords(move):
    """Convert KataGo move format (e.g., 'C3') to SGF coords (e.g., 'cc')."""
    if move.lower() == "pass":
        return ""
    col = move[0].upper()
    row = int(move[1:])
    
    # KataGo uses A-T (no I), SGF uses a-s
    col_map = "ABCDEFGHJKLMNOPQRST"  # Note: no I
    col_idx = col_map.index(col)
    row_idx = row - 1  # SGF is 0-indexed from bottom
    
    # SGF coords: a = column 1, row from top
    # For 9x9, row 1 in KataGo = row 9 in SGF top-down = 'i' (9th letter from top)
    sgf_col = chr(ord('a') + col_idx)
    sgf_row = chr(ord('a') + (8 - row_idx))  # Flip for SGF (top-down)
    
    return sgf_col + sgf_row


def export_puzzle_to_sgf(puzzle_data, filepath):
    """
    Export a puzzle to SGF format.
    """
    sgf_lines = [
        "(;GM[1]FF[4]CA[UTF-8]",
        f"SZ[9]",
        f"GN[Puzzle {puzzle_data['uuid']}]",
        f"PB[{puzzle_data.get('generated_by', 'Unknown')}]",
        f"PW[Opponent]",
        f"C[Generated by Infinite AI Tsumego Miner]",
    ]
    
    # Add setup moves
    for color, move in puzzle_data.get("setup_moves", []):
        sgf_color = "B" if color == "B" else "W"
        sgf_coord = moves_to_sgf_coords(move)
        if sgf_coord:
            sgf_lines.append(f";{sgf_color}[{sgf_coord}]")
    
    # Add comment with solution
    solution = puzzle_data.get("solution", {})
    correct_move = solution.get("correct_move", "?")
    sgf_lines.append(f"C[Puzzle: {puzzle_data['color_to_play']} to play. Solution: {correct_move}]")
    
    sgf_lines.append(")")
    
    with open(filepath, "w") as f:
        f.write("\n".join(sgf_lines))


# =============================================================================
# MATCH ORCHESTRATOR
# =============================================================================

class MatchOrchestrator:
    """
    The conductor of the mining operation.
    
    Manages:
    - The NetworkBench (player agents)  
    - The High Referee (validation engine)
    - The game loop and blunder detection
    - Puzzle extraction and saving
    """
    
    def __init__(self, dry_run=False, log_to_file=False):
        self.dry_run = dry_run
        self.bench = NetworkBench()
        self.referee = None
        self.referee_model_path = None
        
        # Statistics
        self.games_played = 0
        self.puzzles_found = 0
        self.start_time = None
        
        # Deduplication
        self.seen_positions = set()
        
        # Logging
        self.logger = get_logger("Orchestrator")
        self.mining_log = MiningLogger()
    
    def start(self):
        """Initialize all components and begin the mining loop."""
        self.start_time = time.time()
        
        # 1. Initialize the Bench (Players)
        try:
            self.bench.initialize()
        except FileNotFoundError as e:
            self.logger.error(str(e))
            return
        
        if not self.bench.roster:
            self.logger.error("No player models available. Check assets/models/")
            return
        
        # 2. Find and Initialize the Referee
        self.referee_model_path = find_referee_model(MODELS_DIR)
        
        if not self.referee_model_path:
            self.logger.error("No referee model found. Need a strong model for validation.")
            return
        
        referee_info = get_model_info(self.referee_model_path)
        self.logger.info(f">>> Summoning the High Referee: {referee_info['alias']}")
        
        try:
            self.referee = KataGoEngine(
                KATAGO_BIN,
                "configs/referee_config.cfg",
                self.referee_model_path,
                name=referee_info['alias']
            )
        except FileNotFoundError as e:
            self.logger.error(f"Failed to start referee: {e}")
            return
        
        # 3. Ensure output directory exists
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        
        # 4. Dry run mode - just test connectivity
        if self.dry_run:
            self._run_dry_test()
            self.cleanup()
            return
        
        # 5. Main mining loop
        try:
            self.mining_loop()
        except KeyboardInterrupt:
            self.logger.info("\n>>> Mining session interrupted by user.")
        finally:
            self.cleanup()
    
    def _run_dry_test(self):
        """Run a connectivity test without actual mining."""
        self.logger.info("=== DRY RUN MODE ===")
        self.logger.info("Testing engine connectivity...")
        
        # Test referee
        test_result = self.referee.query([])
        if test_result:
            self.logger.info(f"  ✓ Referee responds. Root visits: {test_result['rootInfo']['visits']}")
        else:
            self.logger.error("  ✗ Referee failed to respond")
        
        # Test each player
        for agent in self.bench.roster:
            test_result = agent.engine.query([])
            if test_result:
                self.logger.info(f"  ✓ {agent.alias} responds")
            else:
                self.logger.error(f"  ✗ {agent.alias} failed to respond")
        
        self.logger.info("=== DRY RUN COMPLETE ===")
    
    def mining_loop(self):
        """The main infinite mining loop."""
        self.logger.info(">>> Starting mining loop. Press Ctrl+C to stop.")
        
        while True:
            # A. Select Contenders
            black_agent, white_agent = self.bench.select_matchup()
            self.games_played += 1
            
            self.logger.debug(
                f"Game #{self.games_played}: {black_agent.alias} (B) vs {white_agent.alias} (W)"
            )
            
            # B. Play one game
            puzzles = self.play_match(black_agent, white_agent)
            
            if puzzles:
                self.puzzles_found += len(puzzles)
            
            # C. Status update every 10 games
            if self.games_played % 10 == 0:
                elapsed = time.time() - self.start_time
                rate = (self.puzzles_found / elapsed) * 3600 if elapsed > 0 else 0
                self.mining_log.status_update(self.games_played, self.puzzles_found, rate)
    
    def play_match(self, black_agent, white_agent):
        """
        Simulates a single game between two agents.
        
        Returns:
            List of puzzle data dicts extracted from this game.
        """
        history = []
        current_color = "B"
        consecutive_passes = 0
        puzzles_from_game = []
        
        for turn in range(MAX_MOVES_PER_GAME):
            current_agent = black_agent if current_color == "B" else white_agent
            opponent_color = "W" if current_color == "B" else "B"
            
            # =================================================================
            # 1. REFEREE CHECK (Before the move)
            # Get the "true" evaluation of the current position
            # =================================================================
            referee_analysis = self.referee.query(history)
            if not referee_analysis:
                self.logger.warning("Referee query failed, ending game")
                break
            
            # IMPORTANT: KataGo's rootInfo.winrate is from the perspective of
            # the player TO MOVE. We need to track this carefully.
            # 
            # pre_winrate = probability that the CURRENT PLAYER wins
            pre_winrate = referee_analysis['rootInfo']['winrate']
            
            # Also grab scoreLead (always from Black's perspective)
            pre_score_lead = referee_analysis['rootInfo'].get('scoreLead', 0)
            
            # =================================================================
            # 2. AGENT MOVE
            # Get the agent's chosen move with their personality settings
            # =================================================================
            settings = current_agent.get_query_settings()
            agent_analysis = current_agent.engine.query(history, override_settings=settings)
            
            if not agent_analysis:
                self.logger.warning(f"{current_agent.alias} query failed, ending game")
                break
            
            # Extract the chosen move
            try:
                move_infos = agent_analysis.get('moveInfos', [])
                if not move_infos:
                    break
                chosen_move = move_infos[0]['move']
            except (IndexError, KeyError):
                break
            
            # Handle passes
            if chosen_move.lower() == "pass":
                consecutive_passes += 1
                if consecutive_passes >= MAX_CONSECUTIVE_PASSES:
                    self.logger.debug("Game ended by consecutive passes")
                    break
                history.append([current_color, "pass"])
                current_color = opponent_color
                continue
            else:
                consecutive_passes = 0
            
            # =================================================================
            # 3. BLUNDER DETECTION
            # Check if this move is a catastrophic error
            # =================================================================
            
            # Create hypothetical history with this move
            hypo_history = history + [[current_color, chosen_move]]
            
            # Ask Referee to evaluate AFTER the move
            post_analysis = self.referee.query(hypo_history)
            if not post_analysis:
                # Can't verify, just continue
                history.append([current_color, chosen_move])
                current_color = opponent_color
                continue
            
            # IMPORTANT: After current_color plays, it's now opponent_color's turn.
            # So post_winrate is from the OPPONENT's perspective.
            # To compare apples-to-apples, we need the current player's winrate
            # AFTER their move, which is (1 - opponent's winrate).
            post_opponent_winrate = post_analysis['rootInfo']['winrate']
            post_current_winrate = 1.0 - post_opponent_winrate
            
            # The blunder check: current player was winning big, now losing big
            if pre_winrate > BLUNDER_THRESHOLD_HIGH and post_current_winrate < BLUNDER_THRESHOLD_LOW:
                self.logger.info(
                    f"BLUNDER by {current_agent.alias}: "
                    f"{pre_winrate:.1%} → {post_current_winrate:.1%} after {chosen_move}"
                )
                
                # Check for duplicate position
                pos_hash = hash_position(history)
                if pos_hash in self.seen_positions:
                    self.logger.debug(f"Duplicate position {pos_hash}, skipping")
                else:
                    self.seen_positions.add(pos_hash)
                    
                    # Extract puzzle
                    puzzle = self.extract_puzzle(
                        history=history,
                        color_to_play=current_color,
                        blunder_move=chosen_move,
                        referee_analysis=referee_analysis,
                        post_analysis=post_analysis,
                        agent_alias=current_agent.alias,
                        pre_winrate=pre_winrate,
                        post_winrate=post_current_winrate,
                    )
                    
                    if puzzle:
                        puzzles_from_game.append(puzzle)
                        current_agent.record_blunder()
                
                # Continue playing to find more puzzles in this game
            
            # =================================================================
            # 4. ADVANCE GAME
            # =================================================================
            history.append([current_color, chosen_move])
            current_color = opponent_color
        
        return puzzles_from_game
    
    def extract_puzzle(
        self,
        history,
        color_to_play,
        blunder_move,
        referee_analysis,
        post_analysis,
        agent_alias,
        pre_winrate,
        post_winrate,
    ):
        """
        Extract a puzzle from a detected blunder.
        
        Returns:
            Puzzle data dict, or None if extraction fails.
        """
        puzzle_id = str(uuid.uuid4())[:8]
        
        # Get the correct move (what the Referee thinks is best)
        try:
            correct_move = referee_analysis['moveInfos'][0]['move']
        except (IndexError, KeyError):
            self.logger.warning("Could not determine correct move")
            return None
        
        # Get policy distribution for top moves
        policy_distribution = []
        for i, move_info in enumerate(referee_analysis.get('moveInfos', [])[:5]):
            policy_distribution.append({
                "move": move_info.get("move"),
                "policy": move_info.get("prior", 0),
                "winrate": move_info.get("winrate", 0),
                "visits": move_info.get("visits", 0),
            })
        
        # Get wrong move refutations (top responses to the blunder)
        wrong_move_refutation = []
        for move_info in post_analysis.get('moveInfos', [])[:3]:
            wrong_move_refutation.append({
                "response": move_info.get("move"),
                "winrate": move_info.get("winrate", 0),
            })
        
        # Build puzzle data
        data = {
            "uuid": puzzle_id,
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "generated_by": agent_alias,
            "puzzle_type": "life_and_death_blunder",
            "board_size": 9,
            
            # Position setup
            "setup_moves": history,
            "color_to_play": color_to_play,
            
            # The blunder (for educational purposes)
            "blunder": {
                "move": blunder_move,
                "winrate_before": round(pre_winrate, 4),
                "winrate_after": round(post_winrate, 4),
                "delta": round(pre_winrate - post_winrate, 4),
            },
            
            # The solution
            "solution": {
                "correct_move": correct_move,
                "referee_winrate": round(referee_analysis['rootInfo']['winrate'], 4),
                "referee_visits": referee_analysis['rootInfo']['visits'],
                "policy_prior": referee_analysis['moveInfos'][0].get('prior', 0),
                "difficulty_metrics": {
                    "visits_to_solve": referee_analysis['rootInfo']['visits'],
                    "policy_prior": referee_analysis['moveInfos'][0].get('prior', 0),
                }
            },
            
            # Additional analysis
            "analysis": {
                "policy_distribution": policy_distribution,
                "wrong_move_refutation": wrong_move_refutation,
            },
            
            # Metadata for UI
            "metadata": {
                "ownership": referee_analysis.get('ownership'),
                "score_lead": referee_analysis['rootInfo'].get('scoreLead'),
                "position_hash": hash_position(history),
            }
        }
        
        # Save JSON
        json_path = os.path.join(OUTPUT_DIR, f"puzzle_{puzzle_id}.json")
        with open(json_path, "w") as f:
            json.dump(data, f, indent=2)
        
        # Save SGF
        sgf_path = os.path.join(OUTPUT_DIR, f"puzzle_{puzzle_id}.sgf")
        export_puzzle_to_sgf(data, sgf_path)
        
        self.mining_log.puzzle_saved(puzzle_id, json_path)
        self.logger.debug(f"Also saved SGF: {sgf_path}")
        
        return data
    
    def cleanup(self):
        """Gracefully shut down all engines."""
        self.logger.info(">>> Shutting down engines...")
        
        # Print final stats
        if self.start_time:
            elapsed = time.time() - self.start_time
            self.logger.info(
                f"Session stats: {self.games_played} games, "
                f"{self.puzzles_found} puzzles in {elapsed/60:.1f} minutes"
            )
        
        # Shutdown
        self.bench.shutdown()
        if self.referee:
            self.referee.close()
        
        self.mining_log.shutdown()


# =============================================================================
# CLI ENTRY POINT
# =============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="Infinite AI Tsumego Miner - Generate Go puzzles via adversarial AI self-play"
    )
    
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Test engine connectivity without actual mining"
    )
    
    parser.add_argument(
        "--log-file",
        action="store_true",
        help="Write logs to file in addition to console"
    )
    
    parser.add_argument(
        "--debug",
        action="store_true",
        help="Enable debug logging"
    )
    
    args = parser.parse_args()
    
    # Setup logging
    import logging
    level = logging.DEBUG if args.debug else logging.INFO
    setup_logging(level=level, log_to_file=args.log_file)
    
    # Run
    orchestrator = MatchOrchestrator(dry_run=args.dry_run, log_to_file=args.log_file)
    orchestrator.start()


if __name__ == "__main__":
    main()
